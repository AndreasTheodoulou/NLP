{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP8v5gV5nHQJBoe5kl1Pto",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndreasTheodoulou/NLP/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xIzeL_IZheg9"
      },
      "outputs": [],
      "source": [
        "import pandas   as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Fetch data\n",
        "data = fetch_20newsgroups(subset='all')['data'][:100]"
      ],
      "metadata": {
        "id": "2KR1AUNCDDMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78dmLQybC4jc"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, DistilBertTokenizer\n",
        "\n",
        "# Initialize sentiment analysis pipeline\n",
        "sentiment_model = pipeline(\n",
        "    \"sentiment-analysis\",\n",
        "    model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
        "    device=0  # Use GPU (device=0 for CUDA)\n",
        ")\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Maximum tokens for the model\n",
        "max_tokens = 512\n",
        "\n",
        "def chunk_text(text, max_tokens=512):\n",
        "    \"\"\"\n",
        "    Breaks a long text into chunks of up to max_tokens using tokenizer truncation.\n",
        "    \"\"\"\n",
        "    # Tokenize text and ensure truncation within max_tokens\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "    # Decode tokens back into text chunks strictly limited to max_tokens\n",
        "    return [tokenizer.decode(chunk[:max_tokens], skip_special_tokens=True) for chunk in chunks]\n",
        "\n",
        "# Calculate sentiment scores\n",
        "direction_mapping = {\"POSITIVE\": 1, \"NEGATIVE\": -1}\n",
        "sentiment_scores = []\n",
        "\n",
        "for doc in data:\n",
        "    # Split document into chunks within max_tokens\n",
        "    chunks = chunk_text(doc, max_tokens=max_tokens)\n",
        "\n",
        "    # Process chunks individually to ensure compliance with max_tokens\n",
        "    sentiment_outputs = [sentiment_model(chunk[:max_tokens])[0] for chunk in chunks]\n",
        "\n",
        "    # Calculate sentiment scores and weights for each chunk\n",
        "    sentiment_scores_chunk = [\n",
        "        output[\"score\"] * direction_mapping[output[\"label\"]]\n",
        "        for output in sentiment_outputs\n",
        "    ]\n",
        "    weights_chunk = [len(tokenizer.encode(chunk, add_special_tokens=False)) / max_tokens for chunk in chunks]\n",
        "\n",
        "    # Calculate weighted average sentiment score\n",
        "    weighted_sentiment_score = np.average(sentiment_scores_chunk, weights=weights_chunk)\n",
        "    sentiment_scores.append(weighted_sentiment_score)\n",
        "\n",
        "# Combine data into a DataFrame\n",
        "sentiment_df = data\n",
        "sentiment_df['Sentiment'] = sentiment_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.Sentiment.mean()"
      ],
      "metadata": {
        "id": "eZRJEe0uDXlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_df.Sentiment.hist()"
      ],
      "metadata": {
        "id": "P9vENmGNDZlZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}